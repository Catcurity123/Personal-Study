#### A. S3 Fundamentals
+ Global Storage - `regional` based/resilient
+ Public service, unlimited data & multi-user

![[Pasted image 20231105164254.png]]

![[Pasted image 20231105164848.png]]

###### Note
+ Bucket names are `globally unique`
+ 3-63 characters, all lowercase, no underscores
+ Can't be IP formateed (1.1.1.1)
+ Buckets - 100 soft limit, 1000 hard per account
+ All objects are stored in a flat structure, there is no folders, folders are called `prefix` as it is considered as a part of the name
+ Key = name, Value = data

#### B. S3 Patterns and Anti-Patterns
+ S3 is an object store - not file or block.
+ Cant mount an S3 as (K:\ or /images)
+ Great for large scale data storage, distribution or upload.

#### C. S3 Securities

###### 1. S3 Bucket Policies
(+) S3 Bucket policies is a form of `resource policy`, just like `identity policy`. The main difference is that `resource policy` is assigned to a `resource`, while `identity policy` is assigned to an `identity`.

(+) This provide a `resource perspective permission`, meaning we are controlling `who can access this resource`, while with `identity policy` we are controlling what can that identity access.

(+) Bucket policy can `Allow/Deny` `anonymous` principals, which can't be done using `identity policy`.


###### Note
(+) `Identity policy` has one significant limitation, it can only be attached to identity inside our account, meaning that it can only provide security in our own account.
	==> For example if we want to assign a permission for an entity outside of our account, we must log into the account that controls that entity, there is no way to do it in our account.

(+) `Resource policy`, however, `ALLOW/DENY` same or different accounts, overcoming the limitation of `identity policy`. This is a great way to control the access to a resource no matter the source of access.

###### Example
(+) Another main difference between `Resource policy` and `Identity policy`  is the exisistence of `Principal` field in the policy. For `Identity Policy` it is understood that the `Principal` is the creating account itself, however,  for `Resource policy` we must explicitly provide the `Principal` who can access the resource.

![[Pasted image 20240103145946.png | 600]]

![[Pasted image 20240103150059.png | 600]]

(+) There can only be 1 bucket policy applied to the bucket, but the policy can have multiple statements.


###### 2. Access Control Lists (ACLs)
(+) ACLs on objects and bucket, it is a subresource and is legacy.
(+) The reason is because it is inflexible and simple permission.

![[Pasted image 20240103150522.png]]

###### 3. Block Public Access
(+) This setting is only applied to `public and anonymous principals`. It provide an extra, outter layer of protection for S3 bucket. `Bucket policy`, however, is the inner protection of S3 bucket


![[Pasted image 20240103150918.png | 600]]

(+) `Option 1`: Enable all the other options
(+) `Option 2`: Block public access to newly added buckets or objects and prevent the creation of new ACLs for existing buckets and objects. However, this setting does not change existing permissions that allows public access to S3 using ACLs.
(+) `Option 3`: Block public access to all buckets and objects no matter it is newly created or existing.
(+) `Option 4`: Block public access to newly added buckets or objects. However, this setting does not change existing permissions that allows public access to S3 through policies.
(+) `Option 5`: Block public access and cross-account access to buckets and objects no matter it is newly created or existing.

### Summary for Identity policy and Resource Policy and S3
![[Pasted image 20240103152512.png]]

(+) `Identity Policy` is a good method when we need to control different resources through one centralized platform on one account.
(+) `Bucket Policy or Resource Policy` is a good method when we need to control access right to one product or access can be through anonymous and cross-account principals.

#### D. S3 Static Website Hosting
(+) Normal access is via `AWS APIs`, however, this feature allows access via `HTTP` - e.g Blogs,...
(+) `Index` and `Error` documents are set.
(+) `Website Endpoint` is created.
(+) Custom Domain via `R53` ==> In this case `BUCKETNAME MATTERS`, as the bucket name will also be the domain name for the website

###### Offloading and Out-of-band Pages
(+) S3 Static Website Hosting is usually used for Offloading and Out-of-band pages.
(+) `Offloading` refers to the practice of transferring certain processing tasks from the main CPU to specialized hardware or offloading engine. 
	(-) In the context of AWS, a website can be hosted on an EC2 instance where most of the cost and power is carried out to maintain the functionality of the website, we can offload the delivery of media such as video or pictures to S3, effectively reducing the usage of computing resource on EC2 instance.
(+) `Out-of-band Management (OOB)` refers to using a separate resource site which is distinct from primary data network, this is usually used for troubleshooting and maintenance. 
	(-) In the context of AWS, a website hosted on EC2 may be down for maintenance, but we need to display a maintenance pages for user to know. It would not be feasible to host those pages on EC2 instance which is currently down for troubleshooting, therefore, we can host those pages on S3 as an alternative communication path from the main pages.

![[Pasted image 20240103154558.png | 600]]

#### E. Amazon S3 Pricing

###### 1. For business usage
(+) The cost of `storing data` on S3 is usually expressed as `Per GB/month fee`. Meaning that there is 
(+) The cost of `transferring data` on S3 is also charged. For `transfer data INTO S3 from internet` it is free. For `transfer data OUT OF S3 TO Internet`
(+) The cost of `data requests and retrievals` on S3 is also charged. It is charged per 1000 requests.

###### 2. For Free Tier usage
(+) 5GB of standard storage on S3 is given for free. 20,000 GET request and 2,000 PUT request is for free.

#### Example of hosting static website
(1) Create bucket, enable it to host static website.
(2) Make it public by untick public block access.
(3) Edit bucket policy
```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicRead",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::dixluwn-catpics-bucket.org/*"
        }
    ]
}
```

In the above policy, the `Resource filed has ARN which is Amazon Resource Name`. Colons in this field is used to separate different components of the ARN, and their meaning can vary depending on the AWS service and the resource type. The general format is:

```
arn:partition:service:region:account-id:resource
```

So `aws is partition`, `s3 is service`, and as S3 is a global service it does not need region and account id, hence the 3 colons in the ARN follows by the resource id which is `dixluwn-catpics-bucket.org/*`

Other services may have their own arn

###### Amazon DynamoDB ARN:

```
arn:aws:dynamodb:region:account-id:table/table-name
```

###### AWS Lambda ARN:

```
arn:aws:lambda:region:account-id:function:function-name
```

###### Amazon SNS Topic ARN:

```
arn:aws:sns:region:account-id:topic/topic-name
```

#### F. Object versioning, MFA delete, and Performance Optimization

###### 1. Object versioning
(+) Object versioning is a setting at `bucket level`, it is `disabled` by default, and once it is `enabled` it can not be set to `disabled` again. It can however be `suspended` and `enabled` again if required.

![[Pasted image 20240103191552.png | 600]]

(+) Versioning lets we store `multiple versions` of objects within a bucket. Operations which would modify the objects `generate a new version`.
	(-) Objects in S3 has an `ID` field, when `object version` is `disabled` this field is `null`. When it is enabled, each objects in the bucket will have an `id`. Operation on this object will create a new version of the object with a different ID while retaining the old object.

![[Pasted image 20240103192003.png | 600]]

(+) Deletion will also affect object versioning, when we delete an object without specifying the version, it will create a `delete marker` which will `HIDE` all the object regarding of its version (not delete just hide). We can delete the `delete marker` to restore the object. But if we delete an object while specifying the version, the next most recent object will be come the current object.

![[Pasted image 20240103192438.png]]

(+) If `Object Versioning` is enable, space is consumed by `ALL` versions, and we will be billed for `ALL` versions. Only way to zero out the cost is to delete the bucket as this setting `CAN NOT BE DISABLED` only suspended, and when suspended all the version is still there.

###### 2. MFA Delete
(+) This is enabled in `versioning configuration`, meaning that MFA is required to change `versioning state`. and MFA is required to `delete versions`.
(+) Serial number (MFA) + Code passed with API CALLs is required.

###### 3. S3 Performance Optimization

`Singlle PUT Upload`
(+) The default way of uploading data onto S3 is a `single data stream` from user to s3, meaning that if the stream fails the upload fails and a full restart is required.
(+) The speed and & reliability is limited of 1 streams. This is opposite to downloading data online where it utilizes multiple streams to download and upload data.
(+) Any upload is limited to 5GB.

==> Therefore, distributed transfer of data is being developed to address this issue

![[Pasted image 20240103194827.png]]

(+) There is a solution for this called `Multipart upload`

`Multipart Upload`
(+) Data is broken up, with the min data size of 100MB for multipart. When uploading the maximum parts being broken up is 10,000 max parts, ranging from 5MB -> 5GB.
(+) The last part of `Multipart Upload` can be smaller than 5MB. So parts can fail individually and can be restarted individually.
(+) This means that the transfer rate is equal to the speeds of all parts as they are uploaded individually.

![[Pasted image 20240103195939.png]]

`S3 Accelerated Transfer (OFF)`
![[Pasted image 20240103200114.png]]

`S3 Accelerated Transfer (ON)`
(+) This feature is switched off by default, the limitation of using this feature is that `the bucket name can not contain period` and `bucket name has to be bucketname-compatible`.
(+) The way this works is that the uploaded data is transfered to the closest `Edge Location` not directly to S3. After that `Edge Location` transfer the data over `AWS Global Network` which is maintained by AWS and tend to be a direct link between the `Edge Location` to other AWS's network.

![[Pasted image 20240103200442.png]]

`NOTE`
(+) It is important to understand that the Internet is a `global, interconnect` network with many stops along the way to enable consistency and redundancy. The Internet is much like a public transport with multiple stops for general public usage.
(+) The `AWS Network` though is purposed-built to link one region to another region in the AWS network. So it is much like an express train, meaning that it is faster and with lower consistent latency.

#### G. S3 Encryption
(+) Bucket are not encrypted only objects are encrypted
(+) There are `client-side encryption` and `server-side encryption`. Both of them are encryption at rest.

(+) `Client-side encryption` refers to the practice of the client encrypted the data before uploading onto storage medium (S3 in this case), after uploading onto S3, the data will be encrypted again by AWS and stored on physical S3 storage. During this whole process, AWS can not make sense of any of the uploaded data as it is encrypted the whole time.
(+) `Server-side encryption` refers to the practice of client upload the unencrypted data onto storage medium (S3 in this case), after uploading onto S3, the data will be encrypted by AWS and stored onphysical S3 storage. Although the data will be encrypted and protected during transmission, it is still visable at S3 endpoint.

![[Pasted image 20240104125232.png | 600]]

(+) There are 3 types of `server-side encryption`: `Server-side Encryption with Customer-Provided Keys (SSE-C)`, `Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)`, and `Server-Side Encrytion with Customer Master Keys (CKMs) Stored in AWS Key Management Service (SSE-KMS)`

(+) With `SSE-C`: The customer is responsible for `encryption keys` and AWS responsible for the `encryption and decryption process`. In this case, we are offloading the encryption process (which take computing power) to S3, but we still need to handle the creation and management of `encrypted keys.`
![[Pasted image 20240104125907.png | 600]]

(+) With `SSE-S3 (AES256)`: With this method, S3 manages both the creation and management of keys as well as the encryption and decryption process of the data.
The user only needs to provide S3 with the data. With this method, S3 creates a master key and create for the encryption and decryption process, the user has no influence over this and it is done behind the scene.
(+) This is the standard method of encryption on S3, but it has 3 major drawbacks:
	(-) This is not suitable for regulatory environment where we need to `control access to keys`
	(-) This is not suitable when we need to control the `rotation of keys` according to business logic.
	(-) This is not suitable when we need to define `role-separtion`, as the S3 admin can view the key, and use the key to encrypt or decrypt the data. This is not suitatble as business logic define the `administrative grou`p (which control the system) and `functional group` (which use the system), most of the case the `administrative group` can not perform actions provided for `functional group`.

![[Pasted image 20240104130640.png | 600]]

(+) With `SSE-KMS`: In this case, AWS also handles the creation, management and encryption-decryption process of the data. However, while `SSE-S3` handle encrytion process underline, in this situation we will create a CMK on KMS which will be used to create DEK for every object uploaded on S3. Meaning that whenever we upload data onto S3, S3 will be provided with a DEK created by CMK (created by us) to encrypt the data. Therefore, we can control the management of the CMK (the permission of it using key policies), the key rotation and subsequently achieve role-separation because when we want to decrypt the data using `SSE-KMS` we need to have access to the created CMK.

![[Pasted image 20240104131514.png | 600]]
(+) `Bucket Default Encryption`: If set, objects will use the default encryption if we dont explicitly specifies the encryption we wanted.

(+) `Summary`
![[Pasted image 20240104131811.png | 600]]

#### H. S3 Storage Classes
###### 1. S3 Standard
![[Pasted image 20240104135116.png | 600]]
(+) Objects are replicated accross `at least 3 AZs` in the AWS region
(+) Billed a `GB/m fee` for `data stored`. A `$ per GB` for transfer `OUT` (In is free) and a price `per 1,000 requests`.
(+) Suitable for `Frequently Accessed` data which is `important and non replaceable`.

###### 2. S3 Standard-IA (Infrequent Access)
![[Pasted image 20240104135716.png | 600]]
(+) Same as Standard but it has a `per GB data retrieval fee` meaning frequent data access will increase the cost. It also has a `minimum duration charge of 30 days` and a `minimum capacity charge` of 128KB. Meaning that no matter how short and small your data is and is stored, it will be charged minimum `30 days` and `128KB`, and the bill charged will increase the longer and larger your data is and is stored.
(+) S3 Standard-IA should be used for `long-lived data`, which is `important` but where access is `infrequent`. Dont use it for small data which is stored for a short period and can be easily replaced.

###### 3. S3 One zone-IA
![[Pasted image 20240104140340.png | 600]]
(+) The same as S3 Standard-IA but does not have replication on multiple AZs
(+) Suitable for `long-lived data` which is `non-critical` and `replaceable` and where access is `infrequent`.

###### 4. S3 Glacier
![[Pasted image 20240104141012.png | 600]]
(+) In S3 Glacier, Objects is in `chilled state` meaning that it can not be accessed right away, there is a `retrieval process` that need to be done in order to move that objects to Standard-IA temporarily for viewing. The objects can not be made public either.
(+) There is a `40KB min size` and `90 day min duration` bill for objects in this class. 
(+) S3 Glacier is suitable for `archival data` where `frequent or realtime` access `is not needed`. As it is `miniute-hours first byte latency` different from S3 standard which is `milisecond first byte latency`. But it is one of the cheapest form of storage.

###### 5. S3 Glacier Deep Archive
(+) This class is approximately one-fourth the price of S3 Glacier but with more restriction.
![[Pasted image 20240104141619.png | 600]]
(+) If data in S3 Glacier is in `chilled state` then data in S3 Glacier Deep Archive is in `frozen state` which requires much longer time for retrieval process.
(+) This class is suitable for `archival data` that rarely if ever needs to be accessed - hours of days for retrieval. This is used for legal or regulation data storage of data.

###### 6. S3 Intelligent-Tiering
![[Pasted image 20240104142204.png | 600]]

#### I. S3 Lifecycle Configuration
(+) A lifecycle configuration is a `set of rules consist of actions` on a `Bucket` or `groups of objects` that need `transition actions` into `different states` or `expiration actions`. 

###### 1. Transition flow
![[Pasted image 20240104143107.png | 700]]

###### J. S3 Replication
(+) Replication of S3 allows for `Cross-Region Replication (CRR)` and `Same-Region Replication (SRR)`
![[Pasted image 20240104145611.png | 600]]

(+) Replication configuration is enabled on Source bucket,  a role is needed for S3 to assume so as to perform replication on the destination bucket.
(+) There is one major difference between replication over different account. 
	(-) For replication in the same account, both the source and the destination bucket is owned by the same account, so they both trust account that they are in, they both trust the IAM, and both trust the role. So the role can access both buckets.
	(-) For replication in different account, the destination account is in a different account, so it does not trust the account of the source bucket, meaing that it does not trust the IAM, so it does not trust the role. Meaning the role can not be used to create replication on the destination bucket. To fix this, we need to add a permission in the bucket policy so as to allows the role in the source account to create replication on the destination bucket.

###### S3 Replication Options
(+) We can create replication for all objects or a subset of objects, we can also select the `storage class` for the object.
(+) About the `ownership` of the objects being replicated, the default is that the replicated object is own by the source bucket, meaing that the destination bucket can not view or delete the objects. We can override this so that the ownership is changed to the destination bucket.
(+) `Replication Time Control (RTC)` is to ensure the data between the source and destination is always be in sync.

`NOTE`
(+) Replication is `not retroactive` meaning that the existing data will not be replicated only the newy created.
(+) Replication requires `Versioning to be enabled`.
(+) Replication is `One-way` Source to Destination
(+) Replication support `Unencrypted, SSE-S3, and SSE-KMS`, not `SSE-C` as the keys is not in charge of AWS.
(+) Source bucket needs permission to objects.
(+) Replication do not include system events, objects in Glacier or Glacier Deep Archive.

`Why we use replication`
(+) SRR can be used for log aggregation from different buckets, syncing data from PROD and TEST environment.
(+) SRR might be needed for resilience with strict sovereignty where data must be gathered in one bucket.
(+) CRR can be used for global resilience improvement, or to improve latency reduction for users in different regions.

#### J. S3 Presigned URLs

![[Pasted image 20240104154035.png | 600]]

![[Pasted image 20240104154230.png | 600]]

![[Pasted image 20240104154615.png | 600]]

`NOTE`:
(+) We can create a URL for an object we dont have access to, as the presignedURL will check the permission of the receiving principal, if we dont have access to it, even if we create an URL, we still can not view or perform any operation on it.
(+) When using the URL, the permissions match the `identity which generated it`. Which means even if you have access to the media, if the one who generated the presignedURL does not have access to the media, you still can not use it.
(+) `Do not generate presignedURL with an assumerole` as assumerole will expire for a period of time, which means the presignedURL will also expire. A `long-term identity` is the prefered way of creating presignedURL.

#### K. S3 Select and Glacier Select
(+) S3 can archive a file up to 5TB, when we want to retrieve such file, it will take times and costs for doing so, we can filtered the data after downloading the file, however, this is just for discarding data that we dont need, the whole time and cost is still billed.
(+) The solution for this is using `S3 Select and Glacier Select` which is a `SQL-like` statement service that allow us to filter the needed information and only billed for the needed information.
![[Pasted image 20240105091400.png | 600]]

#### L. S3 Events
###### 1. S3 Event Notifications
(+) `Notification` generated when events occur in a bucket, which can be delivered to `SNS`, `SQS`, and `Lambda` functions for automation tasks. Multiple actions are supported:
	(-) Object `Created` (Put, Post, Copy, CompleteMultiPartUpload).
	(-) Object `Deleted` (`*`, Delete, DeleteMarkerCreated)
	(-) Object `Restore` (Post(Initiated), Completed)
	(-) Replication (OperationMissedThreshold, OperationReplicatedAfterThreshold, OperationNotTracked, OperationFailedReplication)

![[Pasted image 20240105092240.png | 600]]

#### M. S3 Access Logs

![[Pasted image 20240105092610.png | 600]]
